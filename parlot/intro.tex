Understanding and debugging HPC programs 
is time-consuming for the user and computationally inefficient.
%
This is especially true when one has to 
track control flow in terms of function calls and returns that may
span library and system codes. 
%
Traditional software engineering quality assurance methods are 
often inapplicable to HPC where concurrency combined with 
large problem scales and sophisticated domain-specific math can make programming 
very challenging. 
%
For example, it took months for scientists to debug an MPI laser-plasma interaction 
code~\cite{hpcdoe}.


HPC bugs may be a combination of both flawed program logic and unspecified or illegal interactions between various concurrency models (e.g., PThreads, MPI, OpenMP, etc.) that coexist in most large applications \cite{hpcdoe}. Moreover, HPC software tends to consume vast amounts of CPU time and hardware resources. Reproducing bugs by rerunning the application is therefore expensive and undesirable. 
%The best hope for debugging lies in being able to efficiently capture detailed execution 
A natural and field-proven approach for debugging is to capture detailed execution traces and compare the traces against corresponding traces from previous (stable) runs~\cite{stat,cstg}.
%
A {\em key requirement} is to do this collection {\em as efficiently as possible}
and in {\em as general and comprehensive a manner} as possible.


Existing tools in this space
do not meet our criteria for efficiency and generality.
%
The highly acclaimed STAT~\cite{stat} tool has helped isolate
bugs based on building equivalence classes of MPI processes and spotting
outliers.
%
We would like to go beyond the capabilities offered by STAT and support
the collection of {\em whole-program} traces that can then be employed
by a  gamut of back-end tools.
%
Also, STAT is usually brought into the picture
when a failure (e.g., a deadlock or hang) is encountered. We would like
to move toward an ``always on'' collection regime, as we cannot anticipate
when a failure will occur -- or, more importantly, {\em whether the failure
will be reproducible.}
%
There are no reported debugging studies on using STAT in
continuous collection (``always on'') mode.
%
In CSTG~\cite{cstg}, the collection is orchestrated by the
user around chosen collection points and employs heavy-weight
unix {\tt backtrace} calls.
%
These again are different from \parlot, where collection points would not be a priori chosen.


The thrust of the work in this paper is to avoid many of the drawbacks of existing
tracing-based tools.
%
We are interested in avoiding
source-code modifications and recompilation --- thus making binary
instrumentation-based tools the only practical and widely deployable option.
%
We also believe in the value
of creating tools that are {\em portable across a 
wide variety of platforms}.
%

%
Our goal is to use \textit{compression} for trace aggregation and to offer 
a generic and low-overhead tracing method that 
(1)~collects dynamic call information during execution (all function calls and returns) for debugging, performance evaluation, phase detection \cite{cbb}, etc.,
(2)~has low overhead, 
(3)~and requires little tracing bandwidth.
%
{\em Providing all these features in a single tool
that operates based on binary instrumentation
is an unsolved problem.}
%
In this paper, we describe a new tracing approach that fulfills these requirements, which we implemented in our proof-of-concept \parlot tool.


%
With \parlot, users can easily build a host of post-processors to examine
executions from many vantage points.
%
For instance, they can write post-processors
to detect unexpected (or ``outlier'') executions.
%
If needed, they can 
drill down and detect abnormal behaviors {\em even in the runtime and
support library stack} such as MPI-level activities.
%
In HPC, it is well-known (especially on newer machines) that bugs are often due to
broken libraries (MPI, OpenMP), a broken runtime, or OS-level activities.
%
Having a single low-overhead tool that can ``X-ray'' an application to this depth is a goal met by \parlot --- a unique feature in today's tool eco-system.

To further motivate the need for whole-program function call
traces, consider the expression {\tt f()+g()}.
%
In C, there is no sequence point associated with the {\tt +}
operator~\cite{sequence-points-in-C}.
%
If these function calls have inadvertent side-effects causing 
failure, it is important to know in which order {\tt f()}
and {\tt g()} were invoked---something that is easy to discern using
\parlot 's traces.
%
One may be concerned that such a tool introduces excessive execution slowdown.
%
\parlot goes to great lengths to minimize these overheads to a level that we believe most users will find acceptable. The mindset is to \textit{``pay a little upfront to dramatically reduce the number of overall debug iterations''}. 

%
As proof of concept, we gathered preliminary results from using the \parlot tracing mechanism to compare different runs.
%
We injected various bugs into the MPI-related functions of ILCS \cite{ilcs}, a parallelization framework for iterative local searches.
%
We ran \parlot on top of executions of buggy and bug-free versions of ILCS and collected traces.
%
Since \parlot's traces maintain the order of the function calls, we were able to split the traces at multiple points of interest and to feed different chunks of traces to a Concept Lattice data structure \cite{clbook} \cite{clconst}. 
%
Having the totally ordered sequence of function calls of the whole program for each active process/thread enabled us to quickly narrow down the search space to locate the cause of the abnormal behavior in the buggy version of ILCS. 

%
This paper does not pursue debugging per se but rather a thorough benchmarking of \parlot. It makes the following main contributions:
%

\begin{itemize}
\item It introduces a new tracing approach that makes it possible to capture the whole-program call-return, call-stack, call-graph, and call-frequency information, including all library calls, for every thread and process of HPC applications at low overhead in both space and time.
\item It describes a new incremental data compression algorithm to drastically reduce the required tracing bandwidth, thus enabling the collection of whole-program traces, which would be infeasible without on-the-fly compression.
\item It presents \parlot, a proof-of-concept tool that implements our compression-based low-overhead tracing approach. \parlot is capable of instrumenting x86 applications at the binary level (regardless of the source language used) to collect whole-program call traces.
\end{itemize}
%
The remainder of this paper is organized as follows. Section \ref{sec:bgreltool} introduces the basic ideas and infrastructure behind \parlot and other tracing tools. Section \ref{sec:design} describes the design of \parlot in detail. Sections \ref{sec:evalmeth} and \ref{sec:results} present our evaluation of different aspects of \parlot and compare it with a similar tool. Section \ref{sec:concl}
concludes the paper with a summary and future work.
% that includes the construction of verification tools that exploit \parlot traces.




