\subsection{Benchmarks and System}

We performed our evaluations on the MPI-based NAS Parallel Benchmarks (NPB)~\cite{nas}.
%
NPB includes four inputs sizes.
%
To keep the runtimes reasonable, we show results for the class \textit{B} (small-medium) and class \textit{C} (medium-large) inputs.
%

We compiled the NPB codes with the mpicc and mpif77 wrappers of MVAPICH 2.2.1, which are based on icc/ifort 14.0.2 using the prescribed -g and -O1 optimization flags.
%
Quick tests showed that higher optimization levels do not significantly improve the performance.
%

We ran all experiments on Comet at the San Diego Supercomputer Center~\cite{comet}, whose filesystem is NFS and Lustre.
%
Comet has 1944 compute nodes, each of which has dual-socket Intel Xeon E5-2680 v3 processors with a total of 28 cores (14 per socket) and 128 GB of main memory.
%
Note that we only used 16 cores per node as many of the NPB programs require a core count that is a power of two.
%
To study the scaling behavior, we ran experiments on 1, 4, 16 and 64 compute nodes, i.e., on up to 1024 cores.

\subsection{Metrics}

We use the following metrics to quantify and compare the performance of the tracing tools.
%
Unless otherwise noted, all results are based on the median of three identical experiments.
%
\begin{itemize}
\item The \textbf{tracing overhead} is the runtime of the target application when it is being traced divided by the runtime of the same application without tracing.
%
This lower-is-better ratio measures by how much the tracing (and the compression when enabled) slows down the target application.
%
\item The \textbf{tracing bandwidth} is the size of the trace information divided by the application runtime.
%
To make the results easier to compare, we generally list the tracing bandwidth per core, i.e., the tracing bandwidth divided by the number of cores used.
%
This lower-is-better metric is expressed in kilobytes per second (kB/s) per core.
%
It specifies the average needed bandwidth to record the trace data.
%
\item The \textbf{compression ratio} is the size of the uncompressed trace divided by the size of the generated (compressed) trace.
%
This higher-is-better ratio measures the factor by which the compression reduces the trace size.
%
In other words, without compression, the tracing bandwidth would be higher by this factor.
\end{itemize}

\subsection{Tracing Tools}
\label{sec:tracing-tools}

We compare our \parlot tool, implemented on top of \pin 3.5, with \callgrind 3.13.
%
\parlot was compiled with gcc 4.9.2 using \pin 's make system and \callgrind with Valgrind's make system.
%
We created the following versions of \parlot to evaluate different aspects of its design.

\input{tabs.comet.newMed/comet_sd_pMpAcg_BC_itn_p3.5.tex}


\begin{itemize}
\item \textbf{\parlotm} is the normal \parlot tool configured to only collect function-call traces from the main image of the application.
\item \textbf{\parlota} is the normal \parlot tool configured to collect function-call traces from all images of the application, including library function calls.
\item \textbf{\pininit} is a crippled version of \parlot from which the tracing code has been removed.
%
The purpose of \pininit is to see how much of the overhead is due to \pin.
\item \textbf{\parlotnc} is the normal \parlot tool but with compression disabled.
%
It writes out the captured data in uncompressed form.
%
The purpose of \parlotnc is to show the performance impact of the compression.
\end{itemize}

It proved surprisingly difficult to find a tool that is similar to \parlot because there appear to be no other tools that generate whole program call traces.
%
In the end, we settled on \callgrind as the most similar tool we could find and used it for our comparisons.
%
\callgrind is based on the Valgrind DBI tool.
%
It collects function-call graphs combined with performance data to show the user what portion of the execution time has been spent in each function.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FROM RESULTS - average overhead
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Each \callgrind trace file contains a sequence of function names (or their code) plus numerical data for each function on its caller-callee relationship with other functions.
%
Moreover, it contains cost information for each function in terms of how many machine instructions it read.
%
This information is collected using hardware performance counters.
%
The format of the file is plain ASCII text.
%
Interestingly, all numerical values are expressed relative to previous values, i.e., they are delta (or difference) encoded.
%
This simple form of compression is enabled by default in \callgrind.


\begin{figure}[t]
\centering
\includegraphics[width=3.4in,height=1.9in]{figs.comet.newMed/comet_chartAvg_sd_B_p3_5.png}
\caption{Average tracing overhead on the NPB applications - Input B}
\label{comet_chartAvg_sd_B_p3_5}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=3.4in,height=1.9in]{figs.comet.newMed/comet_chartAvg_sd_C_p3_5.png}
\caption{ Average tracing overhead on the NPB applications - Input C}
\label{comet_chartAvg_sd_C_p3_5}
\end{figure}


We believe the information traced by \callgrind is reasonably similar to the information traced by \parlotm.
%
Whereas \callgrind 's traces include performance data that \parlot does not capture, \parlot records the whole-program call trace, which \callgrind does not capture.
%
The full function-call trace is a strict superset of the call-graph information that \callgrind records because the call graph can be extracted from the function-call trace but not vice versa.
%
In particular, \callgrind cannot recreate the order of the function calls a thread made whereas \parlot can.


