
In this paper, 
we present \parlot, a portable low overhead dynamic
binary instrumentation-based
whole-program
tracing approach that can support a variety of 
dynamic program analyses, including debugging.
%
Key properties of \parlot include its on-the-fly trace collection and
compression that reduces timing jitter, I/O bandwidth, and storage requirements to such a degree that whole-program call/return traces can be collected efficiently even at scale. 

We evaluate various versions of \parlot
created by disabling/enabling compression, not collecting any traces, etc.
%
In order to provide an intuitive comparison against a well known tool,
we also compare \parlot to \callgrind.
%
Our metrics include the tracing overhead, required bandwidth, achieved compression ratio, initialization overhead, and the 
overall impact of compression.
%
Detailed evaluations on the NAS parallel benchmarks running on
up to 1024 cores establish the merit of our tool and our design decisions. 
\parlot can collect more than 36 MB worth of data per core per second while 
only needing 56 kB/s of bandwidth and slowing down the 
application by 2.7x on average.
%
These results are highly promising in terms of supporting 
whole program tracing and debugging, in particular when considering that most of the overhead is due to the DBI tool and not \parlot.


The traces collected by \parlot cut through the entire stack of heterogeneous
(MPI, OpenMP, PThreads) calls. 
%
This permits a designer to project these traces onto specific
APIs of interest during program analysis, visualization, and debugging.
%


A number of improvements to \parlot remain to be made.
%
These include allowing users to selectively trace at specific
interfaces: doing so can further increase compression efficiency
by reducing the variety of function calls to be handled by
the compressor.
%
We also discuss the need to bring down initialization overheads, i.e.,
by switching to a less general-purpose DBI tool.
%



\section*{Acknowledgment}

This research was supported by the NSF. We thank our colleague Dr. Hari Sundar from the University of Utah who provided insight and expertise that greatly assisted the research. We also thank the Texas Advanced Computing Center (TACC) and the San Diego Supercomputer Center (SDSC) for the infrastructure they provided for running our experiments.

 
%--end
